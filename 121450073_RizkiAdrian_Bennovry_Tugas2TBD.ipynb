{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuSEjbt6pY7Y",
        "outputId": "a58badb9-ab22-4346-e57d-d6c71044767b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded CIFAR-10 training set:\n",
            " - np.shape(images)     (0,)\n",
            " - np.shape(labels)     (0,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Path to the unzipped CIFAR data\n",
        "data_dir = Path(\"data/cifar-10-batches-py/\")\n",
        "\n",
        "# Unpickle function provided by the CIFAR hosts\n",
        "def unpickle(file):\n",
        "    with open(file, \"rb\") as fo:\n",
        "        dict = pickle.load(fo, encoding=\"bytes\")\n",
        "    return dict\n",
        "\n",
        "images, labels = [], []\n",
        "for batch in data_dir.glob(\"data_batch_*\"):\n",
        "    batch_data = unpickle(batch)\n",
        "    for i, flat_im in enumerate(batch_data[b\"data\"]):\n",
        "        im_channels = []\n",
        "        # Each image is flattened, with channels in order of R, G, B\n",
        "        for j in range(3):\n",
        "            im_channels.append(\n",
        "                flat_im[j * 1024 : (j + 1) * 1024].reshape((32, 32))\n",
        "            )\n",
        "        # Reconstruct the original image\n",
        "        images.append(np.dstack((im_channels)))\n",
        "        # Save the label\n",
        "        labels.append(batch_data[b\"labels\"][i])\n",
        "\n",
        "print(\"Loaded CIFAR-10 training set:\")\n",
        "print(f\" - np.shape(images)     {np.shape(images)}\")\n",
        "print(f\" - np.shape(labels)     {np.shape(labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semua gambar sekarang ada di RAM dalam variabel images, dengan meta data yang sesuai dalam label, dan siap untuk manipulasi. Selanjutnya, menginstal paket Python yang akan digunakan untuk ketiga metode tersebut."
      ],
      "metadata": {
        "id": "ZBIVoCQwrZiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pengaturan untuk Menyimpan Gambar pada Disk\n",
        "Mengatur lingkungan tersebut untuk metode default menyimpan dan mengakses gambar-gambar ini dari disk. Artikel ini akan mengasumsikan bahwa telah menginstal Python 3.x di sistem tersebut, dan akan menggunakan Pillow untuk manipulasi gambar:"
      ],
      "metadata": {
        "id": "074cDuRErlle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtndrkzxqLip",
        "outputId": "51ad9ce3-1c0f-471b-aebd-f65c1ac89e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Melakukan install menggunakan Anaconda:"
      ],
      "metadata": {
        "id": "AI6TgMGir37g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! conda install -c conda-forge pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXyi8DlGskTT",
        "outputId": "cef37ea5-eb48-44ef-e007-a9c363d92ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started With LMDB"
      ],
      "metadata": {
        "id": "aThi8SoBwB7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install lmdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTMfb18RwX4R",
        "outputId": "9fb62c2d-e997-45f0-9c80-0ff96a35c529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.10/dist-packages (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Melakukan instal melalui Anaconda:"
      ],
      "metadata": {
        "id": "Yl9F70awwxnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! conda install -c conda-forge python-lmdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "443FHDEJw0UN",
        "outputId": "dcca2841-47ee-4a1e-f96f-1e066d01932a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started With HDF5"
      ],
      "metadata": {
        "id": "I_Qc6FVkxCRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZUor9RGxG05",
        "outputId": "4d76bb23-f309-40e1-ab60-54df6016653a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menginstal secara bergantian melalui Anaconda:"
      ],
      "metadata": {
        "id": "oggQHzIExXEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! conda install -c conda-forge h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CfvAFVGxY0b",
        "outputId": "01870880-7172-414e-deb3-3cdbd053f90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Storing a Single Image**"
      ],
      "metadata": {
        "id": "jrvlGrdqyAPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Melihat perbandingan kuantitatif dari tugas-tugas dasar yang dipedulikan: berapa lama waktu yang diperlukan untuk membaca dan menulis file, dan berapa banyak memori disk yang akan digunakan. Ini juga akan berfungsi sebagai pengenalan dasar tentang cara kerja metode-metode tersebut, dengan contoh kode tentang cara menggunakannya."
      ],
      "metadata": {
        "id": "iB6uZbdgySHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "disk_dir = Path(\"data/disk/\")\n",
        "lmdb_dir = Path(\"data/lmdb/\")\n",
        "hdf5_dir = Path(\"data/hdf5/\")"
      ],
      "metadata": {
        "id": "5L0jZUQ-yUtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disk_dir.mkdir(parents=True, exist_ok=True)\n",
        "lmdb_dir.mkdir(parents=True, exist_ok=True)\n",
        "hdf5_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "T74YnaWGzDj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing to Disk"
      ],
      "metadata": {
        "id": "ebMW2skAzTF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import csv\n",
        "\n",
        "def store_single_disk(image, image_id, label):\n",
        "    \"\"\" Stores a single image as a .png file on disk.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        image_id    integer unique ID for image\n",
        "        label       image label\n",
        "    \"\"\"\n",
        "    Image.fromarray(image).save(disk_dir / f\"{image_id}.png\")\n",
        "\n",
        "    with open(disk_dir / f\"{image_id}.csv\", \"wt\") as csvfile:\n",
        "        writer = csv.writer(\n",
        "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
        "        )\n",
        "        writer.writerow([label])"
      ],
      "metadata": {
        "id": "8so8Mw9BzXQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing to LMDB"
      ],
      "metadata": {
        "id": "lw1bK0oqzeoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR_Image:\n",
        "    def __init__(self, image, label):\n",
        "        # Dimensions of image for reconstruction - not really necessary\n",
        "        # for this dataset, but some datasets may include images of\n",
        "        # varying sizes\n",
        "        self.channels = image.shape[2]\n",
        "        self.size = image.shape[:2]\n",
        "\n",
        "        self.image = image.tobytes()\n",
        "        self.label = label\n",
        "\n",
        "    def get_image(self):\n",
        "        \"\"\" Returns the image as a numpy array. \"\"\"\n",
        "        image = np.frombuffer(self.image, dtype=np.uint8)\n",
        "        return image.reshape(*self.size, self.channels)"
      ],
      "metadata": {
        "id": "IKmlHqhCzjkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan mengingat ketiga hal tersebut, lihatlah kode untuk menyimpan satu gambar ke LMDB:"
      ],
      "metadata": {
        "id": "cjSAN4owzqHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lmdb\n",
        "import pickle\n",
        "\n",
        "def store_single_lmdb(image, image_id, label):\n",
        "    \"\"\" Stores a single image to a LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        image_id    integer unique ID for image\n",
        "        label       image label\n",
        "    \"\"\"\n",
        "    map_size = image.nbytes * 10\n",
        "\n",
        "    # Create a new LMDB environment\n",
        "    env = lmdb.open(str(lmdb_dir / f\"single_lmdb\"), map_size=map_size)\n",
        "\n",
        "    # Start a new write transaction\n",
        "    with env.begin(write=True) as txn:\n",
        "        # All key-value pairs need to be strings\n",
        "        value = CIFAR_Image(image, label)\n",
        "        key = f\"{image_id:08}\"\n",
        "        txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "T19ozS-Mz7FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing With HDF5"
      ],
      "metadata": {
        "id": "aPMdn5tB0tp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "def store_single_hdf5(image, image_id, label):\n",
        "    \"\"\" Stores a single image to an HDF5 file.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        image_id    integer unique ID for image\n",
        "        label       image label\n",
        "    \"\"\"\n",
        "    # Create a new HDF5 file\n",
        "    file = h5py.File(hdf5_dir / f\"{image_id}.h5\", \"w\")\n",
        "\n",
        "    # Create a dataset in the file\n",
        "    dataset = file.create_dataset(\n",
        "        \"image\", np.shape(image), h5py.h5t.STD_U8BE, data=image\n",
        "    )\n",
        "    meta_set = file.create_dataset(\n",
        "        \"meta\", np.shape(label), h5py.h5t.STD_U8BE, data=label\n",
        "    )\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "IvJw299A0wnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment for Storing Many Images"
      ],
      "metadata": {
        "id": "b600fc_H1Hwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_store_single_funcs = dict(\n",
        "    disk=store_single_disk, lmdb=store_single_lmdb, hdf5=store_single_hdf5\n",
        ")"
      ],
      "metadata": {
        "id": "5O_RWqp11KaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semuanya sudah siap untuk melakukan percobaan waktu. Lalu mencoba menyimpan gambar pertama dari CIFAR dan label yang sesuai, dan menyimpannya dengan tiga cara yang berbeda:"
      ],
      "metadata": {
        "id": "ic6mDSe21lXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import timeit\n",
        "\n",
        "# Example setup for demonstration purposes\n",
        "images = [\"image1\", \"image2\", \"image3\"]  # Replace with actual image data\n",
        "labels = [\"label1\", \"label2\", \"label3\"]  # Replace with actual labels\n",
        "\n",
        "# Example dictionary of functions for demonstration purposes\n",
        "def store_disk(image, idx, label):\n",
        "    pass  # Replace with actual storage logic\n",
        "\n",
        "def store_lmdb(image, idx, label):\n",
        "    pass  # Replace with actual storage logic\n",
        "\n",
        "def store_hdf5(image, idx, label):\n",
        "    pass  # Replace with actual storage logic\n",
        "\n",
        "_store_single_funcs = {\n",
        "    \"disk\": store_disk,\n",
        "    \"lmdb\": store_lmdb,\n",
        "    \"hdf5\": store_hdf5\n",
        "}\n",
        "\n",
        "# Timing dictionary\n",
        "store_single_timings = dict()\n",
        "\n",
        "# Timing each method\n",
        "for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
        "    t = timeit(\n",
        "        \"_store_single_funcs[method](image, 0, label)\",\n",
        "        setup=\"from __main__ import _store_single_funcs, images, labels, method; image=images[0]; label=labels[0]\",\n",
        "        number=1,\n",
        "    )\n",
        "    store_single_timings[method] = t\n",
        "    print(f\"Method: {method}, Time usage: {t}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgQqVnyx3ENI",
        "outputId": "bc1af80d-88f7-4776-a97f-58f70daa3845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: disk, Time usage: 1.4899997040629387e-06\n",
            "Method: lmdb, Time usage: 2.2129997887532227e-06\n",
            "Method: hdf5, Time usage: 1.99999976757681e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Storing Many Images**"
      ],
      "metadata": {
        "id": "zm71jGzF3M-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adjusting the Code for Many Images"
      ],
      "metadata": {
        "id": "OeKVRdER3VSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import lmdb\n",
        "import h5py\n",
        "import csv\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Define directories\n",
        "disk_dir = Path(\"/path/to/disk\")\n",
        "lmdb_dir = Path(\"/path/to/lmdb\")\n",
        "hdf5_dir = Path(\"/path/to/hdf5\")\n",
        "\n",
        "def store_many_disk(images, labels):\n",
        "    \"\"\" Stores an array of images to disk\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        images       images array, (N, 32, 32, 3) to be stored\n",
        "        labels       labels array, (N, 1) to be stored\n",
        "    \"\"\"\n",
        "    num_images = len(images)\n",
        "\n",
        "    # Save all the images one by one\n",
        "    for i, image in enumerate(images):\n",
        "        Image.fromarray(image).save(disk_dir / f\"{i}.png\")\n",
        "\n",
        "    # Save all the labels to the csv file\n",
        "    with open(disk_dir / f\"{num_images}.csv\", \"w\") as csvfile:\n",
        "        writer = csv.writer(\n",
        "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
        "        )\n",
        "        for label in labels:\n",
        "            # This typically would be more than just one value per row\n",
        "            writer.writerow([label])\n",
        "\n",
        "def store_many_lmdb(images, labels):\n",
        "    \"\"\" Stores an array of images to LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        images       images array, (N, 32, 32, 3) to be stored\n",
        "        labels       labels array, (N, 1) to be stored\n",
        "    \"\"\"\n",
        "    num_images = len(images)\n",
        "\n",
        "    map_size = num_images * images[0].nbytes * 10\n",
        "\n",
        "    # Create a new LMDB DB for all the images\n",
        "    env = lmdb.open(str(lmdb_dir / f\"{num_images}_lmdb\"), map_size=map_size)\n",
        "\n",
        "    # Same as before â€” but let's write all the images in a single transaction\n",
        "    with env.begin(write=True) as txn:\n",
        "        for i in range(num_images):\n",
        "            # All key-value pairs need to be Strings\n",
        "            value = (images[i], labels[i])\n",
        "            key = f\"{i:08}\"\n",
        "            txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
        "    env.close()\n",
        "\n",
        "def store_many_hdf5(images, labels):\n",
        "    \"\"\" Stores an array of images to HDF5.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        images       images array, (N, 32, 32, 3) to be stored\n",
        "        labels       labels array, (N, 1) to be stored\n",
        "    \"\"\"\n",
        "    num_images = len(images)\n",
        "\n",
        "    # Create a new HDF5 file\n",
        "    file = h5py.File(hdf5_dir / f\"{num_images}_many.h5\", \"w\")\n",
        "\n",
        "    # Create a dataset in the file\n",
        "    file.create_dataset(\n",
        "        \"images\", np.shape(images), h5py.h5t.STD_U8BE, data=images\n",
        "    )\n",
        "    file.create_dataset(\n",
        "        \"meta\", np.shape(labels), h5py.h5t.STD_U8BE, data=labels\n",
        "    )\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "br0m4dK-32Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Dataset"
      ],
      "metadata": {
        "id": "sQqH9ImZ3_nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cutoffs = [10, 100, 1000, 10000, 100000]\n",
        "\n",
        "# Let's double our images so that we have 100,000\n",
        "images = np.concatenate((images, images), axis=0)\n",
        "labels = np.concatenate((labels, labels), axis=0)\n",
        "\n",
        "# Make sure you actually have 100,000 images and labels\n",
        "print(np.shape(images))\n",
        "print(np.shape(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7wzsnp14m6W",
        "outputId": "a88afe94-e602-4618-d224-edb69e38abaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6,)\n",
            "(6,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_store_many_funcs = dict(\n",
        "    disk=store_many_disk, lmdb=store_many_lmdb, hdf5=store_many_hdf5\n",
        ")\n",
        "\n",
        "from timeit import timeit\n",
        "\n",
        "store_many_timings = {\"disk\": [], \"lmdb\": [], \"hdf5\": []}\n",
        "\n",
        "for cutoff in cutoffs:\n",
        "    for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
        "        t = timeit(\n",
        "            \"_store_many_funcs[method](images_, labels_)\",\n",
        "            setup=\"images_=images[:cutoff]; labels_=labels[:cutoff]\",\n",
        "            number=1,\n",
        "            globals=globals(),\n",
        "        )\n",
        "        store_many_timings[method].append(t)\n",
        "\n",
        "        # Print out the method, cutoff, and elapsed time\n",
        "        print(f\"Method: {method}, Time usage: {t}\")"
      ],
      "metadata": {
        "id": "BcN9juLA61bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grafik pertama menunjukkan waktu penyimpanan normal yang tidak disesuaikan, menyoroti perbedaan drastis antara menyimpan ke file .png dan LMDB atau HDF5.\n",
        "\n",
        "Grafik kedua menunjukkan log pengaturan waktu, menyoroti bahwa HDF5 dimulai lebih lambat daripada LMDB tetapi, dengan jumlah gambar yang lebih besar, sedikit lebih unggul."
      ],
      "metadata": {
        "id": "IsULnZeJ6_iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_with_legend(\n",
        "    x_range, y_data, legend_labels, x_label, y_label, title, log=False\n",
        "):\n",
        "    \"\"\" Displays a single plot with multiple datasets and matching legends.\n",
        "        Parameters:\n",
        "        --------------\n",
        "        x_range         list of lists containing x data\n",
        "        y_data          list of lists containing y values\n",
        "        legend_labels   list of string legend labels\n",
        "        x_label         x axis label\n",
        "        y_label         y axis label\n",
        "    \"\"\"\n",
        "    plt.style.use(\"seaborn-whitegrid\")\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    if len(y_data) != len(legend_labels):\n",
        "        raise TypeError(\n",
        "            \"Error: number of data sets does not match number of labels.\"\n",
        "        )\n",
        "\n",
        "    all_plots = []\n",
        "    for data, label in zip(y_data, legend_labels):\n",
        "        if log:\n",
        "            temp, = plt.loglog(x_range, data, label=label)\n",
        "        else:\n",
        "            temp, = plt.plot(x_range, data, label=label)\n",
        "        all_plots.append(temp)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend(handles=all_plots)\n",
        "    plt.show()\n",
        "\n",
        "# Getting the store timings data to display\n",
        "disk_x = store_many_timings[\"disk\"]\n",
        "lmdb_x = store_many_timings[\"lmdb\"]\n",
        "hdf5_x = store_many_timings[\"hdf5\"]\n",
        "\n",
        "plot_with_legend(\n",
        "    cutoffs,\n",
        "    [disk_x, lmdb_x, hdf5_x],\n",
        "    [\"PNG files\", \"LMDB\", \"HDF5\"],\n",
        "    \"Number of images\",\n",
        "    \"Seconds to store\",\n",
        "    \"Storage time\",\n",
        "    log=False,\n",
        ")\n",
        "\n",
        "plot_with_legend(\n",
        "    cutoffs,\n",
        "    [disk_x, lmdb_x, hdf5_x],\n",
        "    [\"PNG files\", \"LMDB\", \"HDF5\"],\n",
        "    \"Number of images\",\n",
        "    \"Seconds to store\",\n",
        "    \"Log storage time\",\n",
        "    log=True,\n",
        ")"
      ],
      "metadata": {
        "id": "_m5Lwg2g7JBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experiment for Storing Many Images**"
      ],
      "metadata": {
        "id": "rXDF835i7NEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading From Disk"
      ],
      "metadata": {
        "id": "YJkEv71F7jC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_single_disk(image_id):\n",
        "    \"\"\" Stores a single image to disk.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image_id    integer unique ID for image\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        label       associated meta data, int label\n",
        "    \"\"\"\n",
        "    image = np.array(Image.open(disk_dir / f\"{image_id}.png\"))\n",
        "\n",
        "    with open(disk_dir / f\"{image_id}.csv\", \"r\") as csvfile:\n",
        "        reader = csv.reader(\n",
        "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
        "        )\n",
        "        label = int(next(reader)[0])\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "gbhCtDmV7opI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading From LMDB"
      ],
      "metadata": {
        "id": "5aYzzXw-7x35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_single_lmdb(image_id):\n",
        "    \"\"\" Stores a single image to LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image_id    integer unique ID for image\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        label       associated meta data, int label\n",
        "    \"\"\"\n",
        "    # Open the LMDB environment\n",
        "    env = lmdb.open(str(lmdb_dir / f\"single_lmdb\"), readonly=True)\n",
        "\n",
        "    # Start a new read transaction\n",
        "    with env.begin() as txn:\n",
        "        # Encode the key the same way as we stored it\n",
        "        data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
        "        # Remember it's a CIFAR_Image object that is loaded\n",
        "        cifar_image = pickle.loads(data)\n",
        "        # Retrieve the relevant bits\n",
        "        image = cifar_image.get_image()\n",
        "        label = cifar_image.label\n",
        "    env.close()\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "QwjjtZts71gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading From HDF5"
      ],
      "metadata": {
        "id": "YpD9zWpk75gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_single_hdf5(image_id):\n",
        "    \"\"\" Stores a single image to HDF5.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image_id    integer unique ID for image\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        label       associated meta data, int label\n",
        "    \"\"\"\n",
        "    # Open the HDF5 file\n",
        "    file = h5py.File(hdf5_dir / f\"{image_id}.h5\", \"r+\")\n",
        "\n",
        "    image = np.array(file[\"/image\"]).astype(\"uint8\")\n",
        "    label = int(np.array(file[\"/meta\"]).astype(\"uint8\"))\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "KeDJp7dq79Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_read_single_funcs = dict(\n",
        "    disk=read_single_disk, lmdb=read_single_lmdb, hdf5=read_single_hdf5\n",
        ")"
      ],
      "metadata": {
        "id": "jdw8dEx98BoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment for Reading a Single Image"
      ],
      "metadata": {
        "id": "iNqDumgb8IEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import timeit\n",
        "\n",
        "read_single_timings = dict()\n",
        "\n",
        "for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
        "    t = timeit(\n",
        "        \"_read_single_funcs[method](0)\",\n",
        "        setup=\"image=images[0]; label=labels[0]\",\n",
        "        number=1,\n",
        "        globals=globals(),\n",
        "    )\n",
        "    read_single_timings[method] = t\n",
        "    print(f\"Method: {method}, Time usage: {t}\")"
      ],
      "metadata": {
        "id": "pC6rbs7G8ST4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reading Many Images**"
      ],
      "metadata": {
        "id": "axyZwCKT8gh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adjusting the Code for Many Images"
      ],
      "metadata": {
        "id": "PT5LrkWO8mXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_many_disk(num_images):\n",
        "    \"\"\" Reads image from disk.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        num_images   number of images to read\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        images      images array, (N, 32, 32, 3) to be stored\n",
        "        labels      associated meta data, int label (N, 1)\n",
        "    \"\"\"\n",
        "    images, labels = [], []\n",
        "\n",
        "    # Loop over all IDs and read each image in one by one\n",
        "    for image_id in range(num_images):\n",
        "        images.append(np.array(Image.open(disk_dir / f\"{image_id}.png\")))\n",
        "\n",
        "    with open(disk_dir / f\"{num_images}.csv\", \"r\") as csvfile:\n",
        "        reader = csv.reader(\n",
        "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
        "        )\n",
        "        for row in reader:\n",
        "            labels.append(int(row[0]))\n",
        "    return images, labels\n",
        "\n",
        "def read_many_lmdb(num_images):\n",
        "    \"\"\" Reads image from LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        num_images   number of images to read\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        images      images array, (N, 32, 32, 3) to be stored\n",
        "        labels      associated meta data, int label (N, 1)\n",
        "    \"\"\"\n",
        "    images, labels = [], []\n",
        "    env = lmdb.open(str(lmdb_dir / f\"{num_images}_lmdb\"), readonly=True)\n",
        "\n",
        "    # Start a new read transaction\n",
        "    with env.begin() as txn:\n",
        "        # Read all images in one single transaction, with one lock\n",
        "        # We could split this up into multiple transactions if needed\n",
        "        for image_id in range(num_images):\n",
        "            data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
        "            # Remember that it's a CIFAR_Image object\n",
        "            # that is stored as the value\n",
        "            cifar_image = pickle.loads(data)\n",
        "            # Retrieve the relevant bits\n",
        "            images.append(cifar_image.get_image())\n",
        "            labels.append(cifar_image.label)\n",
        "    env.close()\n",
        "    return images, labels\n",
        "\n",
        "def read_many_hdf5(num_images):\n",
        "    \"\"\" Reads image from HDF5.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        num_images   number of images to read\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        images      images array, (N, 32, 32, 3) to be stored\n",
        "        labels      associated meta data, int label (N, 1)\n",
        "    \"\"\"\n",
        "    images, labels = [], []\n",
        "\n",
        "    # Open the HDF5 file\n",
        "    file = h5py.File(hdf5_dir / f\"{num_images}_many.h5\", \"r+\")\n",
        "\n",
        "    images = np.array(file[\"/images\"]).astype(\"uint8\")\n",
        "    labels = np.array(file[\"/meta\"]).astype(\"uint8\")\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "_read_many_funcs = dict(\n",
        "    disk=read_many_disk, lmdb=read_many_lmdb, hdf5=read_many_hdf5\n",
        ")"
      ],
      "metadata": {
        "id": "c7Mg5iSq8sU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment for Reading Many Images"
      ],
      "metadata": {
        "id": "6dltMzPc8t_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import timeit\n",
        "\n",
        "read_many_timings = {\"disk\": [], \"lmdb\": [], \"hdf5\": []}\n",
        "\n",
        "for cutoff in cutoffs:\n",
        "    for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
        "        t = timeit(\n",
        "            \"_read_many_funcs[method](num_images)\",\n",
        "            setup=\"num_images=cutoff\",\n",
        "            number=1,\n",
        "            globals=globals(),\n",
        "        )\n",
        "        read_many_timings[method].append(t)\n",
        "\n",
        "        # Print out the method, cutoff, and elapsed time\n",
        "        print(f\"Method: {method}, No. images: {cutoff}, Time usage: {t}\")"
      ],
      "metadata": {
        "id": "BHDRD9lK8wse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ada beberapa fitur pembeda lain dari LMDB dan HDF5 yang perlu diketahui, dan juga penting untuk membahas secara singkat beberapa kritik terhadap kedua metode tersebut. Beberapa tautan disertakan bersama dengan diskusi jika Anda ingin mempelajari lebih lanjut.\n",
        "\n",
        "- Akses Paralel\\\n",
        "Perbandingan utama yang tidak diuji dalam percobaan di atas adalah pembacaan dan penulisan secara bersamaan. Sering kali, dengan kumpulan data yang besar, mungkin ingin mempercepat operasi melalui paralelisasi.\n",
        "\n",
        "- Dokumentasi\\\n",
        "Jika Google lmdb, setidaknya di Inggris, hasil pencarian ketiga adalah IMDb, Internet Movie Database.\n",
        "\n",
        "- Pandangan yang Lebih Kritis pada Implementasi\\\n",
        "Poin penting yang harus dipahami tentang LMDB adalah bahwa data baru ditulis tanpa menimpa atau memindahkan data yang sudah ada. Ini adalah keputusan desain yang memungkinkan pembacaan yang sangat cepat seperti yang Anda saksikan dalam percobaan kami, dan juga menjamin integritas dan keandalan data tanpa perlu menyimpan catatan transaksi."
      ],
      "metadata": {
        "id": "7PPgVhOg-Lqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Slightly slower\n",
        "for i in range(len(dataset)):\n",
        "    # Read the ith value in the dataset, one at a time\n",
        "    do_something_with(dataset[i])\n",
        "\n",
        "# This is better\n",
        "data = dataset[:]\n",
        "for d in data:\n",
        "    do_something_with(d)"
      ],
      "metadata": {
        "id": "IGOshDqN-ZOY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
